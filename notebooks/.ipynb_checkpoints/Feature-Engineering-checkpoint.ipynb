{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><b>Feature engineering is the process of selecting, transforming, and creating new features from the raw data that can improve the performance of a machine learning model.</b></div></p>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../resources/FE.png\" align='center'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "common tasks\n",
    "\n",
    "- **Detecting and Handling Outliers**\n",
    "- **Missing values Imputation**\n",
    "- **Encoding Categorical Features**\n",
    "- **Feature Scaling**\n",
    "- **Feature Extraction/ Extracting Information**\n",
    "- **Combining Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV Ad Budget ($)</th>\n",
       "      <th>Radio Ad Budget ($)</th>\n",
       "      <th>Newspaper Ad Budget ($)</th>\n",
       "      <th>Sales ($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>38.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>13.8</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>94.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>177.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>283.6</td>\n",
       "      <td>42.0</td>\n",
       "      <td>66.2</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>232.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  TV Ad Budget ($)  Radio Ad Budget ($)  \\\n",
       "0             1             230.1                 37.8   \n",
       "1             2              44.5                 39.3   \n",
       "2             3              17.2                 45.9   \n",
       "3             4             151.5                 41.3   \n",
       "4             5             180.8                 10.8   \n",
       "..          ...               ...                  ...   \n",
       "195         196              38.2                  3.7   \n",
       "196         197              94.2                  4.9   \n",
       "197         198             177.0                  9.3   \n",
       "198         199             283.6                 42.0   \n",
       "199         200             232.1                  8.6   \n",
       "\n",
       "     Newspaper Ad Budget ($)  Sales ($)  \n",
       "0                       69.2       22.1  \n",
       "1                       45.1       10.4  \n",
       "2                       69.3        9.3  \n",
       "3                       58.5       18.5  \n",
       "4                       58.4       12.9  \n",
       "..                       ...        ...  \n",
       "195                     13.8        7.6  \n",
       "196                      8.1        9.7  \n",
       "197                      6.4       12.8  \n",
       "198                     66.2       25.5  \n",
       "199                      8.7       13.4  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv('../datasets/Advertising Budget and Sales.csv')\n",
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and Handling Outliers\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><b>Outliers are data points that are significantly different from other data points in a dataset.</b></div></p>\n",
    "\n",
    "\n",
    "<img src=\"../resources/outliers.jpg\" height=500px width=500px align=left>\n",
    "<img src=\"../resources/outliers.png\" height=500px width=500px align=right>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Outliers\n",
    "\n",
    "- **Univarite-Analysis:**\n",
    "\t- Z-score Method\n",
    "\t- IQR Method\n",
    "\n",
    "- **Bi-variate Analysis**\n",
    "\t- Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-score Method:\n",
    "- The Z-score method looks for data points that are more than three standard deviations away from the mean.\n",
    "\n",
    "#### IQR Method:\n",
    "- The IQR method identifies outliers as data points that fall outside the upper and lower bounds of the IQR range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV Ad Budget ($)</th>\n",
       "      <th>Radio Ad Budget ($)</th>\n",
       "      <th>Newspaper Ad Budget ($)</th>\n",
       "      <th>Sales ($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>67.8</td>\n",
       "      <td>36.6</td>\n",
       "      <td>114.0</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>296.4</td>\n",
       "      <td>36.3</td>\n",
       "      <td>100.9</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  TV Ad Budget ($)  Radio Ad Budget ($)  \\\n",
       "16           17              67.8                 36.6   \n",
       "101         102             296.4                 36.3   \n",
       "\n",
       "     Newspaper Ad Budget ($)  Sales ($)  \n",
       "16                     114.0       12.5  \n",
       "101                    100.9       23.8  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Z_Score Method\n",
    "#formula for Z_score method\n",
    "z_scores = np.abs((sales - sales.mean()) / sales.std())\n",
    "\n",
    "# identify outliers based on z-score threshold\n",
    "threshold = 3\n",
    "outliers = sales[(z_scores > threshold).any(axis=1)]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 'Newspaper Ad Budget ($)'), (101, 'Newspaper Ad Budget ($)')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first quartile (Q1)\n",
    "Q1 = sales.quantile(0.25)\n",
    "\n",
    "#  the third quartile (Q3)\n",
    "Q3 = sales.quantile(0.75)\n",
    "\n",
    "# interquartile range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# threshold for identifying outliers\n",
    "threshold = 1.5\n",
    "\n",
    "# identify outliers based on the threshold\n",
    "outliers = sales[(sales < (Q1 - threshold * IQR)) | (sales > (Q3 + threshold * IQR))].stack().index.tolist()\n",
    "\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers\n",
    "\n",
    "- **Removal/Trimming:** In this method, the outliers are identified and removed from the dataset. This can be done by either deleting the entire row containing the outlier or by replacing the outlier with a new value such as the mean or median of the dataset.\n",
    "\n",
    "- **Imputation:** In this method, the outliers are replaced with a new value such as the mean, median, or mode of the dataset.\n",
    "\n",
    "- **Winsorization:** In this method, the outliers are replaced with a value at a certain percentile. For example, if the 95th percentile is used, all values above the 95th percentile are replaced with the value at the 95th percentile.\n",
    "\n",
    "- **Transformation:** In this method, the data is transformed in a way that reduces the effect of outliers. This can be done by applying a log transformation, a square root transformation, or a Box-Cox transformation.\n",
    "\n",
    "- **Binning:** In this method, the data is divided into bins or intervals and the outliers are replaced with the upper or lower limit of the bin.\n",
    "\n",
    "- **Clipping:** In this method, the outliers are replaced with a value at a certain threshold. For example, if the threshold is set at 3 standard deviations from the mean, all values above or below this threshold are replaced with the threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling outliers using `mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here write your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handling outliers using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values Imputation\n",
    "\n",
    "<img src=\"../resources/Imputation Techniques types.jpeg\" height=500px width=400px align=left>\n",
    "<img src=\"../resources/imputation.png\" width=400px height=500px align=right>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **Mean/median imputation:** In this method, missing values are replaced with the mean or median of the available values in the same column. This method is simple to implement and can work well when missing values are randomly distributed. However, it can lead to biased models if missing values are not randomly distributed.\n",
    "\n",
    "- **Mode imputation:** This method is used for categorical data and replaces missing values with the most common value in the same column. It is simple and easy to implement but can also lead to biased models if the missing values are not randomly distributed.\n",
    "\n",
    "- **Hot-deck imputation:** This method replaces missing values with values from other similar observations in the same dataset. It works well when missing values are missing at random but can lead to biased models if the missing values are related to the missing observations.\n",
    "\n",
    "- **K-NN imputation:** In this method, missing values are replaced with values from the K nearest neighbors in the same dataset. It can work well when the missing values are related to the available observations but can lead to biased models if the missing values are related to the missing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a dataset of 10 rows and 6 columns\n",
    "data = {'Feature_1': [12, 15, 20, 10, 30, 17, 25, 22, 19, 27],\n",
    "        'Feature_2': [1500, 1700, 1800, 1200, 2000, 1600, np.nan, 1900, 1700, 2100],\n",
    "        'Feature_3': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B', 'A', 'C'],\n",
    "        'Feature_4': [3, 7, 5, 9, 6, 4, 8, 2, np.nan, 7],\n",
    "        'Feature_5': [0.75, 0.9, 1.2, 0.5, 1.5, 1.1, 1.3, 1.0, 0.8, 1.4],\n",
    "        'Feature_6': [300, 200, 400, 500, 350, 250, 450, 600, 350, 550]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n",
    "\n",
    "<img src=\"../resources/encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Label Encoding** is used when we have ordinal categorical variables, which means there is some order or ranking in the categories. In Label Encoding, each unique category is assigned a numerical value starting from 0, 1, 2 and so on, based on their order or ranking. `For example`, if we have a variable \"Size\" with categories 'Small', 'Medium', and 'Large', then we can assign values of 0, 1, and 2 to these categories respectively. Label Encoding is advantageous when we have a large number of categories, as it reduces the number of unique values in the variable and makes it easier to work with. However, it can introduce an arbitrary ordering in the data, which can affect the performance of the model.\n",
    "\n",
    "- **One-Hot Encoding** is used when we have nominal categorical variables, which means there is no order or ranking in the categories. In One-Hot Encoding, each unique category is converted into a new binary feature, where the presence of the category is represented by a value of 1 and the absence of the category is represented by a value of 0. `For example`, if we have a variable \"Color\" with categories 'Red', 'Green', and 'Blue', then we can create three new binary features 'Color_Red', 'Color_Green', and 'Color_Blue', where each feature will have a value of 1 if the original category was present and 0 if it was absent. One-Hot Encoding is advantageous as it preserves the information of the categories without introducing an arbitrary ordering. However, it can lead to the curse of dimensionality, where the number of features in the dataset becomes very large, making it computationally expensive and difficult to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers in Feature_1\n",
    "df['Feature_1'] = np.where(df['Feature_1'] > 25, 25, df['Feature_1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values in Feature_2 with mean\n",
    "df['Feature_2'].fillna(df['Feature_2'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical features in Feature_3\n",
    "df = pd.concat([df, pd.get_dummies(df['Feature_3'], prefix='Feature_3')], axis=1)\n",
    "df.drop('Feature_3', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scaling features with Min-Max scaling in Feature_5 and Feature_6\n",
    "df['Feature_5'] = (df['Feature_5'] - df['Feature_5'].min()) / (df['Feature_5'].max() - df['Feature_5'].min())\n",
    "df['Feature_6'] = (df['Feature_6'] - df['Feature_6'].min()) / (df['Feature_6'].max() - df['Feature_6'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting information from Feature_1 by squaring the values\n",
    "df['Feature_1_squared'] = df['Feature_1'] ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining information from Feature_4 and Feature_6\n",
    "df['Feature_4_times_6'] = df['Feature_4'] * df['Feature_6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>B</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>C</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>C</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>C</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_1  Feature_2 Feature_3  Feature_4  Feature_5  Feature_6\n",
       "0         12     1500.0         A        3.0       0.75        300\n",
       "1         15     1700.0         B        7.0       0.90        200\n",
       "2         20     1800.0         A        5.0       1.20        400\n",
       "3         10     1200.0         C        9.0       0.50        500\n",
       "4         30     2000.0         B        6.0       1.50        350\n",
       "5         17     1600.0         C        4.0       1.10        250\n",
       "6         25        NaN         A        8.0       1.30        450\n",
       "7         22     1900.0         B        2.0       1.00        600\n",
       "8         19     1700.0         A        NaN       0.80        350\n",
       "9         27     2100.0         C        7.0       1.40        550"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Here are some common tasks that are performed in feature engineering:\n",
    "\n",
    "\n",
    "\n",
    "- **Feature Selection:** This involves selecting the most relevant features from the raw data that can improve the performance of the model. This can be done using techniques such as correlation analysis, mutual information, or statistical tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Select the top n features based on correlation\n",
    "n = 10\n",
    "top_features = corr_matrix.nlargest(n, 'target')['target'].index\n",
    "selected_features = data[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Select the top n features based on mutual information\n",
    "n = 10\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=n)\n",
    "selected_features = selector.fit_transform(data.drop('target', axis=1), data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Feature Scaling:** This involves scaling the features to a similar range to avoid bias in the model. This can be done using techniques such as normalization or standardization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('target', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('target', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature Encoding:** This involves representing categorical features as numerical features that can be used by the model. This can be done using techniques such as one-hot encoding or label encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# One-hot encode the categorical features\u001b[39;00m\n\u001b[1;32m      7\u001b[0m encoded_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat_feature\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "#Using one-hot encoding\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "encoded_features = pd.get_dummies(data, columns=['cat_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Label encode the categorical features\n",
    "encoder = LabelEncoder()\n",
    "encoded_features = data.copy()\n",
    "encoded_features['cat_feature'] = encoder.fit_transform(data['cat_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature Transformation:** This involves transforming the features to a different space that can improve the performance of the model. This can be done using techniques such as PCA or LDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Apply PCA to the features\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components)\n",
    "transformed_features = pca.fit_transform(data.drop('target', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Apply LDA to the features\n",
    "n_components = 2\n",
    "lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "transformed_features = lda.fit_transform(data.drop('target', axis=1), data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature Creation:** This involves creating new features from the raw data that can capture important patterns or relationships in the data. This can be done using techniques such as polynomial features or interaction features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Create polynomial features\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "new_features = poly.fit_transform(data.drop('target', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Create interaction features\n",
    "new_feature = data['feature1'] * data['feature2']\n",
    "data['new_feature'] = new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Feature Extraction:** This involves extracting useful information from the raw data that can be used as features. This can be done using techniques such as text or image feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Extract features from text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "text_features = vectorizer.fit_transform(data['text_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread('image.jpg')\n",
    "\n",
    "# Extract features from the image\n",
    "features = np.array(img).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Feature Discretization:** This involves discretizing continuous features into categorical features that can be used by the model. This can be done using techniques such as binning or quantile-based discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Discretize the numerical feature using binning\n",
    "bins = [0, 10, 20, 30, 40, 50]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "data['binned_feature'] = pd.cut(data['numerical_feature'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Discretize the numerical feature using quantile-based discretization\n",
    "n_bins = 5\n",
    "data['quantile_feature'] = pd.qcut(data['numerical_feature'], q=n_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature engineering pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "    ('kbest', SelectKBest(score_func=mutual_info_classif, k=5))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, ['numeric_feature1', 'numeric_feature2']),\n",
    "        ('cat', categorical_transformer, ['categorical_feature'])\n",
    "    ])\n",
    "\n",
    "# Define the final pipeline with feature engineering and machine learning model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the testing data\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Neural Network Basics Like Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step-by-step derivation of forward propagation for a neural network with one input layer, two hidden layers, and one output layer. Let's assume that there are $n^{[0]}$ input features, $n^{[1]}$ neurons in the first hidden layer, $n^{[2]}$ neurons in the second hidden layer, and $n^{[3]}$ neurons in the output layer. We will use the following notation:\n",
    "\n",
    "- $a^{[0]} \\in \\mathbb{R}^{n^{[0]}}$ is the input vector\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$ is the weight matrix for layer $l$\n",
    "- $b^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ is the bias vector for layer $l$\n",
    "- $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$ is the pre-activation vector for layer $l$\n",
    "- $a^{[l]} = g^{[l]}(z^{[l]})$ is the activation vector for layer $l$, where $g^{[l]}$ is the activation function for layer $l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notation, the forward propagation algorithm for this neural network can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{[1]} &= W^{[1]}a^{[0]} + b^{[1]} \\\n",
    "a^{[1]} &= g^{[1]}(z^{[1]}) \\\n",
    "z^{[2]} &= W^{[2]}a^{[1]} + b^{[2]} \\\n",
    "a^{[2]} &= g^{[2]}(z^{[2]}) \\\n",
    "z^{[3]} &= W^{[3]}a^{[2]} + b^{[3]} \\\n",
    "a^{[3]} &= g^{[3]}(z^{[3]})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's derive the mathematical steps for each layer:\n",
    "\n",
    "First Hidden Layer:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{[1]} &= W^{[1]}a^{[0]} + b^{[1]} \\\n",
    "& = \\begin{bmatrix}\n",
    "w_{11}^{[1]} & w_{12}^{[1]} & \\cdots & w_{1n^{[0]}}^{[1]} \\\n",
    "w_{21}^{[1]} & w_{22}^{[1]} & \\cdots & w_{2n^{[0]}}^{[1]} \\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\n",
    "w_{n^{[1]}1}^{[1]} & w_{n^{[1]}2}^{[1]} & \\cdots & w_{n^{[1]}n^{[0]}}^{[1]}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_1^{[0]} \\\n",
    "a_2^{[0]} \\\n",
    "\\vdots \\\n",
    "a_{n^{[0]}}^{[0]}\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{bmatrix}\n",
    "b_1^{[1]} \\\n",
    "b_2^{[1]} \\\n",
    "\\vdots \\\n",
    "b_{n^{[1]}}^{[1]}\n",
    "\\end{bmatrix} \\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}^{[1]}a_1^{[0]} + w_{12}^{[1]}a_2^{[0]} + \\cdots + w_{1n^{[0]}}^{[1]}a_{n^{[0]}}^{[0]} + b_1^{[1]} \\\n",
    "w_{21}^{[1]}a_1^{[0]} + w_{22}^{[1]}a_2^{[0]} + \\cdots + w_{2n^{[0]}}^{[1]}a_{n^{[0]}}^{[0]} + b_2^{[1]} \\\n",
    "\\vdots \\\n",
    "w_{n^{[1]}1}^{[1]}a_1^{[0]} + w_{n^{[1]}2}^{[1]}a_2^{[0]} + \\cdots + w_{n^{[1]}n^{[0]}}^{[1]}a_{n^{[0]}}^{[0]} + b_{n^{[1]}}^{[1]}\n",
    "\\end{bmatrix} \\\n",
    "a^{[1]} &= g^{[1]}(z^{[1]}) \\\n",
    "&= \\begin{bmatrix}\n",
    "g^{[1]}(z_1^{[1]}) \\\n",
    "g^{[1]}(z_2^{[1]}) \\\n",
    "\\vdots \\\n",
    "g^{[1]}(z_{n^{[1]}}^{[1]})\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Hidden Layer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{[2]} &= W^{[2]}a^{[1]} + b^{[2]} \\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}^{[2]} & w_{12}^{[2]} & \\cdots & w_{1n^{[1]}}^{[2]} \\\n",
    "w_{21}^{[2]} & w_{22}^{[2]} & \\cdots & w_{2n^{[1]}}^{[2]} \\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\n",
    "w_{n^{[2]}1}^{[2]} & w_{n^{[2]}2}^{[2]} & \\cdots & w_{n^{[2]}n^{[1]}}^{[2]}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_1^{[1]} \\\n",
    "a_2^{[1]} \\\n",
    "\\vdots \\\n",
    "a_{n^{[1]}}^{[1]}\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "b_1^{[2]} \\\n",
    "b_2^{[2]} \\\n",
    "\\vdots \\\n",
    "b_{n^{[2]}}^{[2]}\n",
    "\\end{bmatrix} \\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}^{[2]}a_1^{[1]} + w_{12}^{[2]}a_2^{[1]} + \\cdots + w_{1n^{[1]}}^{[2]}a_{n^{[1]}}^{[1]} + b_1^{[2]} \\\n",
    "w_{21}^{[2]}a_1^{[1]} + w_{22}^{[2]}a_2^{[1]} + \\cdots + w_{2n^{[1]}}^{[2]}a_{n^{[1]}}^{[1]} + b_2^{[2]} \\\n",
    "\\vdots \\\n",
    "w_{n^{[2]}1}^{[2]}a_1^{[1]} + w_{n^{[2]}2}^{[2]}a_2^{[1]} + \\cdots + w_{n^{[2]}n^{[1]}}^{[2]}a_{n^{[1]}}^{[1]} + b_{n^{[2]}}^{[2]}\n",
    "\\end{bmatrix} \\\n",
    "a^{[2]} &= g^{[2]}(z^{[2]}) \\\n",
    "&= \\begin{bmatrix}\n",
    "g^{[2]}(z_1^{[2]}) \\\n",
    "g^{[2]}(z_2^{[2]}) \\\n",
    "\\vdots \\\n",
    "g^{[2]}(z_{n^{[2]}}^{[2]})\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Layer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{[3]} &= W^{[3]}a^{[2]} + b^{[3]} \\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}^{[3]} & w_{12}^{[3]} & \\cdots & w_{1n^{[2]}}^{[3]} \\\n",
    "w_{21}^{[3]} & w_{22}^{[3]} & \\cdots & w_{2n^{[2]}}^{[3]} \\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\n",
    "w_{n^{[3]}1}^{[3]} & w_{n^{[3]}2}^{[3]} & \\cdots & w_{n^{[3]}n^{[2]}}^{[3]}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_1^{[2]} \\\n",
    "a_2^{[2]} \\\n",
    "\\vdots \\\n",
    "a_{n^{[2]}}^{[2]}\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "b_1^{[3]} \\\n",
    "b_2^{[3]} \\\n",
    "\\vdots \\\n",
    "b_{n^{[3]}}^{[3]}\n",
    "\\end{bmatrix} \\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}^{[3]}a_1^{[2]} + w_{12}^{[3]}a_2^{[2]} + \\cdots + w_{1n^{[2]}}^{[3]}a_{n^{[2]}}^{[2]} + b_1^{[3]} \\\n",
    "w_{21}^{[3]}a_1^{[2]} + w_{22}^{[3]}a_2^{[2]} + \\cdots + w_{2n^{[2]}}^{[3]}a_{n^{[2]}}^{[2]} + b_2^{[3]} \\\n",
    "\\vdots \\\n",
    "w_{n^{[3]}1}^{[3]}a_1^{[2]} + w_{n^{[3]}2}^{[3]}a_2^{[2]} + \\cdots + w_{n^{[3]}n^{[2]}}^{[3]}a_{n^{[2]}}^{[2]} + b_{n^{[3]}}^{[3]}\n",
    "\\end{bmatrix} \\\n",
    "a^{[3]} &= g^{[3]}(z^{[3]}) \\\n",
    "&= \\begin{bmatrix}\n",
    "g^{[3]}(z_1^{[3]}) \\\n",
    "g^{[3]}(z_2^{[3]}) \\\n",
    "\\vdots \\\n",
    "g^{[3]}(z_{n^{[3]}}^{[3]})\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This completes the derivation for the forward propagation algorithm for a neural network with one input layer, two hidden layers, and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's a step-by-step derivation of backpropagation for a neural network with one input layer, two hidden layers, and one output layer. Let's assume that there are $n^{[0]}$ input features, $n^{[1]}$ neurons in the first hidden layer, $n^{[2]}$ neurons in the second hidden layer, and $n^{[3]}$ neurons in the output layer. We will use the following notation:\n",
    "\n",
    "- $a^{[0]} \\in \\mathbb{R}^{n^{[0]}}$ is the input vector\n",
    "- $W^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$ is the weight matrix for layer $l$\n",
    "- $b^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ is the bias vector for layer $l$\n",
    "- $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$ is the pre-activation vector for layer $l$\n",
    "- $a^{[l]} = g^{[l]}(z^{[l]})$ is the activation vector for layer $l$, where $g^{[l]}$ is the activation function for layer $l$\n",
    "- $y$ is the true output value for the given input $a^{[0]}$\n",
    "- $J$ is the cost function, which measures the error between the predicted output $\\hat{y}$ and the true output $y$\n",
    "\n",
    "With this notation, the backpropagation algorithm for this neural network can be written as:\n",
    "\n",
    "- Compute the output layer error: $\\delta^{[3]} = \\nabla_{\\hat{y}} J \\odot g^{[3]'}(z^{[3]})$\n",
    "- Compute the second hidden layer error: $\\delta^{[2]} = (W^{[3]})^T \\delta^{[3]} \\odot g^{[2]'}(z^{[2]})$\n",
    "- Compute the first hidden layer error: $\\delta^{[1]} = (W^{[2]})^T \\delta^{[2]} \\odot g^{[1]'}(z^{[1]})$\n",
    "- Compute the gradients for the output layer weights and biases: $\\nabla_{W^{[3]}} J = \\delta^{[3]} (a^{[2]})^T$, $\\nabla_{b^{[3]}} J = \\delta^{[3]}$\n",
    "- Compute the gradients for the second hidden layer weights and biases: $\\nabla_{W^{[2]}} J = \\delta^{[2]} (a^{[1]})^T$, $\\nabla_{b^{[2]}} J = \\delta^{[2]}$\n",
    "- Compute the gradients for the first hidden layer weights and biases: $\\nabla_{W^{[1]}} J = \\delta^{[1]} (a^{[0]})^T$, $\\nabla_{b^{[1]}} J = \\delta^{[1]}$\n",
    "- Update the weights and biases for all layers using the computed gradients and a learning rate $\\alpha$:\n",
    "        \n",
    "$W^{[l]} := W^{[l]} - \\alpha \\nabla_{W^{[l]}} J$\n",
    "\n",
    "\n",
    "$b^{[l]} := b^{[l]} - \\alpha \\nabla_{b^{[l]}} J$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's compute the mathematical steps for each of these steps:\n",
    "\n",
    "- Compute the output layer error:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\delta^{[3]} &= \\nabla_{\\hat{y}} J \\odot g^{[3]'}(z^{[3]}) \\\n",
    "    &= (\\hat{y} - y) \\odot g^{[3]'}(z^{[3]})\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the second hidden layer error:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\delta^{[2]} &= (W^{[3]})^T \\delta^{[3]} \\odot g^{[2]'}(z^{[2]}) \\\n",
    "    &= (W^{[3]})^T (\\delta^{[3]} \\odot g^{[2]'}(z^{[2]}))\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the first hidden layer error:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\delta^{[1]} &= (W^{[2]})^T \\delta^{[2]} \\odot g^{[1]'}(z^{[1]}) \\\n",
    "    &= (W^{[2]})^T (\\delta^{[2]} \\odot g^{[1]'}(z^{[1]}))\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the gradients for the output layer weights and biases:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\nabla_{W^{[3]}} J &= \\delta^{[3]} (a^{[2]})^T \\\n",
    "    \\nabla_{b^{[3]}} J &= \\delta^{[3]}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the gradients for the second hidden layer weights and biases:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\nabla_{W^{[2]}} J &= \\delta^{[2]} (a^{[1]})^T \\\n",
    "    \\nabla_{b^{[2]}} J &= \\delta^{[2]}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the gradients for the first hidden layer weights and biases:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\nabla_{W^{[1]}} J &= \\delta^{[1]} (a^{[0]})^T \\\n",
    "    \\nabla_{b^{[1]}} J &= \\delta^{[1]}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Update the weights and biases for all layers using the computed gradients and a learning rate $\\alpha$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    W^{[l]} &:= W^{[l]} - \\alpha \\nabla_{W^{[l]}} J \\\n",
    "    b^{[l]} &:= b^{[l]} - \\alpha \\nabla_{b^{[l]}} J\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    for $l=1,2,3$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's a step-by-step derivation of backpropagation for a neural network with one input layer having two input nodes/neurons, two hidden layers both containing 3 nodes, and one output layer. Let's use the following notation:\n",
    "\n",
    "- $a^{[0]} \\in \\mathbb{R}^{2}$ is the input vector\n",
    "- $W^{[1]} \\in \\mathbb{R}^{3 \\times 2}$ is the weight matrix for the first hidden layer\n",
    "- $b^{[1]} \\in \\mathbb{R}^{3}$ is the bias vector for the first hidden layer\n",
    "- $z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$ is the pre-activation vector for the first hidden layer\n",
    "- $a^{[1]} = g^{[1]}(z^{[1]})$ is the activation vector for the first hidden layer, where $g^{[1]}$ is the activation function for the first hidden layer\n",
    "- $W^{[2]} \\in \\mathbb{R}^{3 \\times 3}$ is the weight matrix for the second hidden layer\n",
    "- $b^{[2]} \\in \\mathbb{R}^{3}$ is the bias vector for the second hidden layer\n",
    "- $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$ is the pre-activation vector for the second hidden layer\n",
    "- $a^{[2]} = g^{[2]}(z^{[2]})$ is the activation vector for the second hidden layer, where $g^{[2]}$ is the activation function for the second hidden layer\n",
    "- $W^{[3]} \\in \\mathbb{R}^{1 \\times 3}$ is the weight matrix for the output layer\n",
    "- $b^{[3]} \\in \\mathbb{R}$ is the bias vector for the output layer\n",
    "- $z^{[3]} = W^{[3]}a^{[2]} + b^{[3]}$ is the pre-activation scalar for the output layer\n",
    "- $y$ is the true output value for the given input $a^{[0]}$\n",
    "- $J$ is the cost function, which measures the error between the predicted output $\\hat{y}$ and the true output $y$\n",
    "\n",
    "With this notation, the backpropagation algorithm for this neural network can be written as:\n",
    "\n",
    "- Compute the output layer error: $\\delta^{[3]} = \\nabla_{\\hat{y}} J \\odot g^{[3]'}(z^{[3]})$\n",
    "- Compute the second hidden layer error: $\\delta^{[2]} = (W^{[3]})^T \\delta^{[3]} \\odot g^{[2]'}(z^{[2]})$\n",
    "- Compute the first hidden layer error: $\\delta^{[1]} = (W^{[2]})^T \\delta^{[2]} \\odot g^{[1]'}(z^{[1]})$\n",
    "- Compute the gradients for the output layer weights and biases: $\\nabla_{W^{[3]}} J = \\delta^{[3]} (a^{[2]})^T$, $\\nabla_{b^{[3]}} J = \\delta^{[3]}$\n",
    "- Compute the gradients for the second hidden layer weights and biases: $\\nabla_{W^{[2]}} J = \\delta^{[2]} (a^{[1]})^T$, $\\nabla_{b^{[2]}} J = \\delta^{[2]}$\n",
    "- Compute the gradients for the first hidden layer weights and biases: $\\nabla_{W^{[1]}} J = \\delta^{[1]} (a^{[0]})^T$, $\\nabla_{b^{[1]}} J = \\delta^{[1]}$\n",
    "- Update the weights and biases for all layers using the computed gradients and a learning rate $\\alpha$:\n",
    "        \n",
    "$W^{[l]} := W^{[l]} - \\alpha \\nabla_{W^{[l]}} J$\n",
    "                                                                                          \n",
    "                                                                                          \n",
    "$b^{[l]} := b^{[l]} - \\alpha \\nabla_{b^{[l]}} J$\n",
    "\n",
    "Now, let's compute the mathematical steps for each of these steps:\n",
    "\n",
    "- Compute the output layer error:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\delta^{[3]} &= \\nabla_{\\hat{y}} J \\odot g^{[3]'}(z^{[3]}) \\\n",
    "    &= (\\hat{y} - y) \\odot g^{[3]'}(z^{[3]})\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the second hidden layer error:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\delta^{[2]} &= (W^{[3]})^T \\delta^{[3]} \\odot g^{[2]'}(z^{[2]}) \\\n",
    "    &= (W^{[3]})^T (\\delta^{[3]} \\odot g^{[2]'}(z^{[2]}))\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the first hidden layer error:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\delta^{[1]} &= (W^{[2]})^T \\delta^{[2]} \\odot g^{[1]'}(z^{[1]}) \\\n",
    "    &= (W^{[2]})^T (\\delta^{[2]} \\odot g^{[1]'}(z^{[1]}))\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the gradients for the output layer weights and biases:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\nabla_{W^{[3]}} J &= \\delta^{[3]} (a^{[2]})^T \\\n",
    "    \\nabla_{b^{[3]}} J &= \\delta^{[3]}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the gradients for the second hidden layer weights and biases:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\nabla_{W^{[2]}} J &= \\delta^{[2]} (a^{[1]})^T \\\n",
    "    \\nabla_{b^{[2]}} J &= \\delta^{[2]}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Compute the gradients for the first hidden layer weights and biases:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\nabla_{W^{[1]}} J &= \\delta^{[1]} (a^{[0]})^T \\\n",
    "    \\nabla_{b^{[1]}} J &= \\delta^{[1]}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Update the weights and biases for all layers using the computed gradients and a learning rate $\\alpha$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    W^{[l]} &:= W^{[l]} - \\alpha \\nabla_{W^{[l]}} J \\\n",
    "    b^{[l]} &:= b^{[l]} - \\alpha \\nabla_{b^{[l]}} J\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    for $l=1,2,3$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's a step-by-step derivation of forward propagation for a neural network with one input layer having two input nodes/neurons, two hidden layers both containing three nodes, and one output layer. Let's use the same notation as in the previous answer:\n",
    "\n",
    "- $a^{[0]} \\in \\mathbb{R}^{2}$ is the input vector\n",
    "- $W^{[1]} \\in \\mathbb{R}^{3 \\times 2}$ is the weight matrix for the first hidden layer\n",
    "- $b^{[1]} \\in \\mathbb{R}^{3}$ is the bias vector for the first hidden layer\n",
    "- $z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$ is the pre-activation vector for the first hidden layer\n",
    "- $a^{[1]} = g^{[1]}(z^{[1]})$ is the activation vector for the first hidden layer, where $g^{[1]}$ is the activation function for the first hidden layer\n",
    "- $W^{[2]} \\in \\mathbb{R}^{3 \\times 3}$ is the weight matrix for the second hidden layer\n",
    "- $b^{[2]} \\in \\mathbb{R}^{3}$ is the bias vector for the second hidden layer\n",
    "- $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$ is the pre-activation vector for the second hidden layer\n",
    "- $a^{[2]} = g^{[2]}(z^{[2]})$ is the activation vector for the second hidden layer, where $g^{[2]}$ is the activation function for the second hidden layer\n",
    "- $W^{[3]} \\in \\mathbb{R}^{1 \\times 3}$ is the weight matrix for the output layer\n",
    "- $b^{[3]} \\in \\mathbb{R}$ is the bias vector for the output layer\n",
    "- $z^{[3]} = W^{[3]}a^{[2]} + b^{[3]}$ is the pre-activation scalar for the output layer\n",
    "- $y$ is the true output value for the given input $a^{[0]}$\n",
    "\n",
    "With this notation, the forward propagation algorithm for this neural network can be written as:\n",
    "\n",
    "- Compute the pre-activation vector for the first hidden layer: $z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$\n",
    "- Compute the activation vector for the first hidden layer: $a^{[1]} = g^{[1]}(z^{[1]})$\n",
    "- Compute the pre-activation vector for the second hidden layer: $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$\n",
    "- Compute the activation vector for the second hidden layer: $a^{[2]} = g^{[2]}(z^{[2]})$\n",
    "- Compute the pre-activation scalar for the output layer: $z^{[3]} = W^{[3]}a^{[2]} + b^{[3]}$\n",
    "- Compute the predicted output value: $\\hat{y} = g^{[3]}(z^{[3]})$\n",
    "\n",
    "Now, let's compute the mathematical steps for each of these steps:\n",
    "\n",
    "- Compute the pre-activation vector for the first hidden layer:\n",
    "    $$\n",
    "    z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}\n",
    "    $$\n",
    "\n",
    "- Compute the activation vector for the first hidden layer:\n",
    "    $$\n",
    "    a^{[1]} = g^{[1]}(z^{[1]})\n",
    "    $$\n",
    "\n",
    "- Compute the pre-activation vector for the second hidden layer:\n",
    "    $$\n",
    "    z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}\n",
    "    $$\n",
    "\n",
    "- Compute the activation vector for the second hidden layer:\n",
    "    $$\n",
    "    a^{[2]} = g^{[2]}(z^{[2]})\n",
    "    $$\n",
    "\n",
    "- Compute the pre-activation scalar for the output layer:\n",
    "    $$\n",
    "    z^{[3]} = W^{[3]}a^{[2]} + b^{[3]}\n",
    "    $$\n",
    "\n",
    "- Compute the predicted output value:\n",
    "    $$\n",
    "    \\hat{y} = g^{[3]}(z^{[3]})\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define the weight matrices and bias vectors for each layer. We'll use random numerical values for demonstration purposes.\n",
    "\n",
    "\\begin{align}\n",
    "W_1 = \\begin{bmatrix}\n",
    "0.2 & 0.4 \\\n",
    "0.1 & 0.3 \\\n",
    "0.5 & 0.7 \\\n",
    "\\end{bmatrix} &&\n",
    "b_1 = \\begin{bmatrix}\n",
    "0.1 \\\n",
    "0.2 \\\n",
    "0.3 \\\n",
    "\\end{bmatrix} \\\n",
    "W_2 = \\begin{bmatrix}\n",
    "0.4 & 0.6 & 0.8 \\\n",
    "0.3 & 0.5 & 0.7 \\\n",
    "\\end{bmatrix} &&\n",
    "b_2 = \\begin{bmatrix}\n",
    "0.4 \\\n",
    "0.5 \\\n",
    "\\end{bmatrix} \\\n",
    "W_3 = \\begin{bmatrix}\n",
    "0.9 & 0.2 & 0.1 \\\n",
    "0.3 & 0.8 & 0.5 \\\n",
    "0.5 & 0.7 & 0.2 \\\n",
    "\\end{bmatrix} &&\n",
    "b_3 = \\begin{bmatrix}\n",
    "0.6 \\\n",
    "0.7 \\\n",
    "0.8 \\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Next, let's assume we have an input vector $\\mathbf{x}$ of size (2,1). We'll use the following values for demonstration purposes:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x} = \\begin{bmatrix}\n",
    "0.9 \\\n",
    "0.1 \\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The first step is to calculate the activations of the first hidden layer:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_1 = W_1 \\mathbf{x} + b_1 \\\n",
    "\\mathbf{a}_1 = \\sigma(\\mathbf{z}_1)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{align}\n",
    "\n",
    "Substituting in the values:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_1 = \\begin{bmatrix}\n",
    "0.2 & 0.4 \\\n",
    "0.1 & 0.3 \\\n",
    "0.5 & 0.7 \\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0.9 \\\n",
    "0.1 \\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "0.1 \\\n",
    "0.2 \\\n",
    "0.3 \\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.74 \\\n",
    "0.92 \\\n",
    "1.78 \\\n",
    "\\end{bmatrix} \\\n",
    "\\mathbf{a}_1 = \\sigma(\\mathbf{z}_1) = \\begin{bmatrix}\n",
    "0.676 \\\n",
    "0.715 \\\n",
    "0.855 \\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Next, we calculate the activations of the second hidden layer:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_2 = W_2 \\mathbf{a}_1 + b_2 \\\n",
    "\\mathbf{a}_2 = \\sigma(\\mathbf{z}_2)\n",
    "\\end{align}\n",
    "\n",
    "Substituting in the values:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_2 = \\begin{bmatrix}\n",
    "0.4 & 0.6 & 0.8 \\\n",
    "0.3 & 0.5 & 0.7 \\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0.676 \\\n",
    "0.715 \\\n",
    "0.855 \\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "0.4 \\\n",
    "0.5 \\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.265 \\\n",
    "1.450 \\\n",
    "\\end{bmatrix} \\\n",
    "\\mathbf{a}_2 = \\sigma(\\mathbf{z}_2) = \\begin{bmatrix}\n",
    "0.779 \\\n",
    "0.810 \\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Finally, we calculate the activations of the output layer:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_3 = W_3 \\mathbf{a}_2 + b_3 \\\n",
    "\\mathbf{a}_3 = \\sigma(\\mathbf{z}_3)\n",
    "\\end{align}\n",
    "\n",
    "Substituting in the values:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_3 = \\begin{bmatrix}\n",
    "0.9 & 0.2 & 0.1 \\\n",
    "0.3 & 0.8 & 0.5 \\\n",
    "0.5 & 0.7 & 0.2 \\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "0.779 \\\n",
    "0.810 \\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "0.6 \\\n",
    "0.7 \\\n",
    "0.8 \\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.743 \\\n",
    "1.862 \\\n",
    "1.357 \\\n",
    "\\end{bmatrix} \\\n",
    "\\mathbf{a}_3 = \\sigma(\\mathbf{z}_3) = \\begin{bmatrix}\n",
    "0.850 \\\n",
    "0.865 \\\n",
    "0.795 \\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, the output of the neural network for the given input is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a}_3 = \\begin{bmatrix}\n",
    "0.850 \\\n",
    "0.865 \\\n",
    "0.795 \\\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's assume that the input to the network is a vector $\\mathbf{x} = [x_1, x_2]^T$, and the output is a scalar $y$. We will use the sigmoid activation function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ for all the hidden and output neurons.\n",
    "\n",
    "First, we need to initialize the weights and biases of the network. Let $w_{ij}^{(l)}$ be the weight of the connection from neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$. Let $b_j^{(l)}$ be the bias of neuron $j$ in layer $l$. We will use random initialization for the weights and biases. Let's assume that the initial weights and biases are:\n",
    "\n",
    "$w_{11}^{(1)} = 0.2$, $w_{12}^{(1)} = -0.3$, $b_1^{(1)} = -0.4$, $w_{21}^{(1)} = 0.1$, $w_{22}^{(1)} = 0.4$, $b_2^{(1)} = 0.2$\n",
    "\n",
    "$w_{11}^{(2)} = -0.5$, $w_{12}^{(2)} = 0.3$, $w_{13}^{(2)} = 0.1$, $b_1^{(2)} = 0.1$, $w_{21}^{(2)} = 0.2$, $w_{22}^{(2)} = -0.1$, $w_{23}^{(2)} = -0.2$, $b_2^{(2)} = -0.1$\n",
    "\n",
    "$w_{11}^{(3)} = 0.3$, $w_{12}^{(3)} = -0.2$, $w_{13}^{(3)} = -0.1$, $b^{(3)} = 0.2$\n",
    "\n",
    "Next, we need to feed the input $\\mathbf{x}$ forward through the network to compute the output $y$ and the activations of all the neurons. Let $z_j^{(l)}$ be the weighted sum of the inputs to neuron $j$ in layer $l$, and let $a_j^{(l)}$ be the activation of neuron $j$ in layer $l$. Then the forward pass equations are:\n",
    "\n",
    "$z_1^{(1)} = w_{11}^{(1)}x_1 + w_{21}^{(1)}x_2 + b_1^{(1)} = 0.2 \\times x_1 + 0.1 \\times x_2 - 0.4$\n",
    "\n",
    "$a_1^{(1)} = \\sigma(z_1^{(1)})$\n",
    "\n",
    "$z_2^{(1)} = w_{12}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)} = -0.3 \\times x_1 + 0.4 \\times x_2 + 0.2$\n",
    "\n",
    "$a_2^{(1)} = \\sigma(z_2^{(1)})$\n",
    "\n",
    "$z_1^{(2)} = w_{11}^{(2)}a_1^{(1)} + w_{21}^{(2)}a_2^{(1)} + w_{31}^{(2)}a_3^{(1)} + b_1^{(2)} = -0.5a_1^{(1)} + 0.2a_2^{(1)} + 0.1a_3^{(1)} + 0.1$\n",
    "\n",
    "$a_1^{(2)} = \\sigma(z_1^{(2)})$\n",
    "\n",
    "$z_2^{(2)} = w_{12}^{(2)}a_1^{(1)} + w_{22}^{(2)}a_2^{(1)} + w_{32}^{(2)}a_3^{(1)} + b_2^{(2)} = 0.3a_1^{(1)} - 0.1a_2^{(1)} - 0.2a_3^{(1)} - 0.1$\n",
    "\n",
    "$a_2^{(2)} = \\sigma(z_2^{(2)})$\n",
    "\n",
    "$z_3^{(2)} = w_{13}^{(2)}a_1^{(1)} + w_{23}^{(2)}a_2^{(1)} + w_{33}^{(2)}a_3^{(1)} + b_3^{(2)} = 0.1a_1^{(1)} - 0.2a_2^{(1)} - 0.1a_3^{(1)} - 0.1$\n",
    "\n",
    "$a_3^{(2)} = \\sigma(z_3^{(2)})$\n",
    "\n",
    "$z_1^{(3)} = w_{11}^{(3)}a_1^{(2)} + w_{21}^{(3)}a_2^{(2)} + w_{31}^{(3)}a_3^{(2)} + b^{(3)} = 0.3a_1^{(2)} - 0.2a_2^{(2)} - 0.1a_3^{(2)} + 0.2$\n",
    "\n",
    "$a^{(3)} = \\sigma(z_1^{(3)}) = y$\n",
    "\n",
    "Now, we can compute the error between the predicted output $y$ and the true output $y_{\\text{true}}$. Let $E$ be the mean squared error:\n",
    "\n",
    "$E = \\frac{1}{2}(y - y_{\\text{true}})^2$\n",
    "\n",
    "We want to minimize this error by adjusting the weights and biases of the network. To do this, we will use the backpropagation algorithm to compute the gradients of the error with respect to the weights and biases.\n",
    "\n",
    "First, we compute the derivative of the error with respect to the output activation:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y} = y - y_{\\text{true}}$\n",
    "\n",
    "Next, we compute the derivative of the output activation with respect to its input:\n",
    "\n",
    "$\\frac{\\partial y}{\\partial z_1^{(3)}} = \\sigma(z_1^{(3)})(1 - \\sigma(z_1^{(3)}))$\n",
    "\n",
    "Using the chain rule, we can compute the derivative of the error with respect to the input $z_1^{(3)}$:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_1^{(3)}} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z_1^{(3)}} = (y - y_{\\text{true}}) \\cdot \\sigma(z_1^{(3)})(1 - \\sigma(z_1^{(3)}))$\n",
    "\n",
    "Next, we can use the derivatives of the activations and inputs of the neurons in the output layer to compute the derivatives of the error with respect to the weights and biases in the output layer:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{ij}^{(3)}} = \\frac{\\partial E}{\\partial z_1^{(3)}} \\cdot \\frac{\\partial z_1^{(3)}}{\\partial w_{ij}^{(3)}} = \\frac{\\partial E}{\\partial z_1^{(3)}} \\cdot a_j^{(2)}$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b^{(3)}} = \\frac{\\partial E}{\\partial z_1^{(3)}} \\cdot \\frac{\\partial z_1^{(3)}}{\\partial b^{(3)}} = \\frac{\\partial E}{\\partial z_1^{(3)}}$\n",
    "\n",
    "Now, we need to propagate the error derivatives backwards through the network to compute the gradients for the hidden layers. Let's start with the second hidden layer. We can compute the derivative of the error with respect to the input $z_j^{(2)}$ of neuron $j$ in the second hidden layer using the chain rule:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_j^{(2)}} = \\sum_k \\frac{\\partial E}{\\partial z_k^{(3)}} \\cdot \\frac{\\partial z_k^{(3)}}{\\partial a_j^{(2)}} \\cdot \\frac{\\partial a_j^{(2)}}{\\partial z_j^{(2)}} = \\sum_k \\frac{\\partial E}{\\partial z_k^{(3)}} \\cdot w_{jk}^{(3)} \\cdot \\sigma(z_j^{(2)})(1 - \\sigma(z_j^{(2)}))$\n",
    "\n",
    "Using the derivative of the activation function, we can compute the derivatives of the error with respect to the weights and biases in the second hidden layer:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{ij}^{(2)}} = \\frac{\\partial E}{\\partial z_j^{(2)}} \\cdot \\frac{\\partial z_j^{(2)}}{\\partial w_{ij}^{(2)}} = \\frac{\\partial E}{\\partial z_j^{(2)}} \\cdot a_i^{(1)}$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b_j^{(2)}} = \\frac{\\partial E}{\\partial z_j^{(2)}} \\cdot \\frac{\\partial z_j^{(2)}}{\\partial b_j^{(2)}} = \\frac{\\partial E}{\\partial z_j^{(2)}}$\n",
    "\n",
    "Similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the derivatives for the first hidden layer. We can compute the derivative of the error with respect to the input $z_j^{(1)}$ of neuron $j$ in the first hidden layer using the chain rule:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial z_j^{(1)}} = \\sum_k \\frac{\\partial E}{\\partial z_k^{(2)}} \\cdot \\frac{\\partial z_k^{(2)}}{\\partial a_j^{(1)}} \\cdot \\frac{\\partial a_j^{(1)}}{\\partial z_j^{(1)}} = \\sum_k \\frac{\\partial E}{\\partial z_k^{(2)}} \\cdot w_{jk}^{(2)} \\cdot \\sigma(z_j^{(1)})(1 - \\sigma(z_j^{(1)}))$\n",
    "\n",
    "Using the derivative of the activation function, we can compute the derivatives of the error with respect to the weights and biases in the first hidden layer:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{ij}^{(1)}} = \\frac{\\partial E}{\\partial z_j^{(1)}} \\cdot \\frac{\\partial z_j^{(1)}}{\\partial w_{ij}^{(1)}} = \\frac{\\partial E}{\\partial z_j^{(1)}} \\cdot x_i$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b_j^{(1)}} = \\frac{\\partial E}{\\partial z_j^{(1)}} \\cdot \\frac{\\partial z_j^{(1)}}{\\partial b_j^{(1)}} = \\frac{\\partial E}{\\partial z_j^{(1)}}$\n",
    "\n",
    "Now we have computed all the gradients necessary to update the weights and biases of the network. Let's use a learning rate $\\alpha$ to control the size of the updates. The weight and bias updates are:\n",
    "\n",
    "$w_{ij}^{(l)} \\leftarrow w_{ij}^{(l)} - \\alpha \\frac{\\partial E}{\\partial w_{ij}^{(l)}}$\n",
    "\n",
    "$b_j^{(l)} \\leftarrow b_j^{(l)} - \\alpha \\frac{\\partial E}{\\partial b_j^{(l)}}$\n",
    "\n",
    "We repeat this process for multiple iterations until the error is minimized to an acceptable level.\n",
    "\n",
    "Note that the above steps can be summarized as follows:\n",
    "\n",
    "- Initialize weights and biases randomly\n",
    "- Feed input forward through the network to compute output and activations\n",
    "- Compute error and its derivative with respect to output activation\n",
    "- Compute derivatives of error with respect to weights and biases in output layer\n",
    "- Propagate error derivatives backwards through the network to compute derivatives for hidden layers\n",
    "- Use the gradients to update the weights and biases of the network\n",
    "- Repeat steps 2-6 for multiple iterations until error is minimized to an acceptable level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
